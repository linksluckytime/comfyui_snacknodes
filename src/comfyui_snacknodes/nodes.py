import os
import torch
import math
import numpy as np
from PIL import Image, ImageOps
import locale


class ImageScaler:
    CATEGORY = "SnackNodes"  # Node group name

    @classmethod
    def get_language(cls):
        try:
            lang = locale.getdefaultlocale()[0]
            return 'zh' if lang and lang.startswith('zh') else 'en'
        except:
            return 'en'

    @classmethod
    def INPUT_TYPES(cls):
        lang = cls.get_language()
        
        # Define options
        if lang == 'zh':
            pixel_budget_options = ["512åƒç´ å¹³æ–¹", "1024åƒç´ å¹³æ–¹"]
            scale_factor_options = [2, 8, 32, 64]
            reference_position_options = [
                "å±…ä¸­", "å·¦ä¸Š", "ä¸­ä¸Š", "å³ä¸Š",
                "å·¦ä¸­", "ä¸­ä¸­", "å³ä¸­",
                "å·¦ä¸‹", "ä¸­ä¸‹", "å³ä¸‹"
            ]
            supersampling_methods = [
                "æ— ", "å…°ç´¢æ–¯", "æœ€è¿‘é‚»", "åŒçº¿æ€§", "åŒä¸‰æ¬¡", "ç›’å¼", "æ±‰æ˜"
            ]
            padding_options = ["é€æ˜", "ç°è‰²", "é»‘è‰²", "ç™½è‰²"]
        else:
            pixel_budget_options = ["512pxÂ²", "1024pxÂ²"]
            scale_factor_options = [2, 8, 32, 64]
            reference_position_options = [
                "Center", "Top-Left", "Top-Center", "Top-Right",
                "Middle-Left", "Middle-Center", "Middle-Right",
                "Bottom-Left", "Bottom-Center", "Bottom-Right"
            ]
            supersampling_methods = [
                "None", "Lanczos", "Nearest", "Bilinear", "Bicubic", "Box", "Hamming"
            ]
            padding_options = ["Transparent", "Gray", "Black", "White"]

        if lang == 'zh':
            return {
                "required": {
                    "image": ("IMAGE", {"description": "è¾“å…¥å›¾åƒå¼ é‡ (B,H,W,C æ ¼å¼)"}),
                    "preserve_aspect_ratio": ("BOOLEAN", {
                        "default": True,
                        "description": "ä¿æŒå®½é«˜æ¯”"
                    }),
                    "pixel_budget": (pixel_budget_options, {
                        "default": "1024åƒç´ å¹³æ–¹",
                        "description": "æœ€å¤§åƒç´ é¢ç§¯"
                    }),
                    "scale_factor": (scale_factor_options, {
                        "default": 64,
                        "description": "å°ºå¯¸å€æ•°"
                    }),
                    "reference_position": (reference_position_options, {
                        "default": "å±…ä¸­",
                        "description": "ä½ç½®å‚è€ƒç‚¹"
                    }),
                    "supersampling_method": (supersampling_methods, {
                        "default": "æ— ",
                        "description": "é‡é‡‡æ ·æ–¹æ³•"
                    }),
                    "enable_padding": ("BOOLEAN", {
                        "default": False,
                        "description": "å¯ç”¨å¡«å……"
                    }),
                    "padding": (padding_options, {
                        "default": "é€æ˜",
                        "description": "å¡«å……é¢œè‰²"
                    }),
                },
            }
        else:
            return {
                "required": {
                    "image": ("IMAGE", {"description": "Input image tensor (B,H,W,C format)"}),
                    "preserve_aspect_ratio": ("BOOLEAN", {
                        "default": True,
                        "description": "Maintain aspect ratio"
                    }),
                    "pixel_budget": (pixel_budget_options, {
                        "default": "1024pxÂ²",
                        "description": "Max pixel area"
                    }),
                    "scale_factor": (scale_factor_options, {
                        "default": 64,
                        "description": "Dimension multiple"
                    }),
                    "reference_position": (reference_position_options, {
                        "default": "Center",
                        "description": "Position reference"
                    }),
                    "supersampling_method": (supersampling_methods, {
                        "default": "None",
                        "description": "Resampling method"
                    }),
                    "enable_padding": ("BOOLEAN", {
                        "default": False,
                        "description": "Enable padding"
                    }),
                    "padding": (padding_options, {
                        "default": "Transparent",
                        "description": "Padding color"
                    }),
                },
            }

    RETURN_TYPES = ("IMAGE", "INT", "INT")
    RETURN_NAMES = ("image", "width", "height")
    FUNCTION = "scale_image"

    def scale_image(self, image, preserve_aspect_ratio, pixel_budget, scale_factor, reference_position, supersampling_method, enable_padding, padding):
        # Map Chinese options to English for internal processing
        if self.get_language() == 'zh':
            pixel_budget_map = {
                "512åƒç´ å¹³æ–¹": "512pxÂ²",
                "1024åƒç´ å¹³æ–¹": "1024pxÂ²"
            }
            reference_position_map = {
                "å±…ä¸­": "Center", "å·¦ä¸Š": "Top-Left", "ä¸­ä¸Š": "Top-Center", "å³ä¸Š": "Top-Right",
                "å·¦ä¸­": "Middle-Left", "ä¸­ä¸­": "Middle-Center", "å³ä¸­": "Middle-Right",
                "å·¦ä¸‹": "Bottom-Left", "ä¸­ä¸‹": "Bottom-Center", "å³ä¸‹": "Bottom-Right"
            }
            supersampling_map = {
                "æ— ": "None", "å…°ç´¢æ–¯": "Lanczos", "æœ€è¿‘é‚»": "Nearest", "åŒçº¿æ€§": "Bilinear",
                "åŒä¸‰æ¬¡": "Bicubic", "ç›’å¼": "Box", "æ±‰æ˜": "Hamming"
            }
            padding_map = {
                "é€æ˜": "Transparent", "ç°è‰²": "Gray", "é»‘è‰²": "Black", "ç™½è‰²": "White"
            }
            
            pixel_budget = pixel_budget_map.get(pixel_budget, pixel_budget)
            reference_position = reference_position_map.get(reference_position, reference_position)
            supersampling_method = supersampling_map.get(supersampling_method, supersampling_method)
            padding = padding_map.get(padding, padding)

        # Validate inputs
        max_pixels = {"512pxÂ²": 512 ** 2, "1024pxÂ²": 1024 ** 2}.get(pixel_budget)
        assert max_pixels, f"Invalid pixel budget: {pixel_budget}"

        # Extract image dimensions
        batch_size, height, width, channels = image.shape
        img = Image.fromarray((image[0].numpy() * 255).astype("uint8"))

        # Ensure image is in RGBA mode for transparent padding
        if padding == "Transparent":
            img = img.convert("RGBA")

        # Calculate target dimensions
        if preserve_aspect_ratio:
            # Case 1: Maintain aspect ratio
            new_width, new_height = self.calculate_dimensions(width, height, max_pixels, scale_factor)
            img = img.resize((new_width, new_height), self.get_interpolation_method(supersampling_method))
        else:
            # Case 2: Don't maintain aspect ratio
            target_size = math.isqrt(max_pixels)
            target_size = (target_size // scale_factor) * scale_factor

            # Check if image can fit within target size while maintaining aspect ratio
            can_fit = (width <= target_size and height <= target_size) or \
                     (width * target_size / height <= target_size and height * target_size / width <= target_size)

            if not can_fit:
                # Case 2.1: Image cannot fit - crop
                img = self.crop_image(img, target_size, target_size, reference_position)
            else:
                if enable_padding:
                    # Case 2.2.1: Image can fit and padding is enabled
                    img = self.scale_with_padding(img, target_size, target_size, reference_position, 
                                                supersampling_method, padding)
                else:
                    # Case 2.2.2: Image can fit but padding is disabled - stretch
                    img = img.resize((target_size, target_size), self.get_interpolation_method(supersampling_method))

        # Convert back to tensor
        output_image = torch.from_numpy(np.array(img).astype(np.float32)) / 255.0
        output_image = output_image.unsqueeze(0)

        # Send success notification
        self.send_notification(f"Image scaled to {img.width}x{img.height}")
        return output_image, img.width, img.height

    @staticmethod
    def calculate_dimensions(width, height, max_pixels, scale_factor):
        aspect_ratio = width / height
        new_width = min(width, math.isqrt(int(max_pixels * aspect_ratio)))
        new_height = min(height, math.isqrt(int(max_pixels / aspect_ratio)))

        # Align dimensions to scale factor
        new_width = (new_width // scale_factor) * scale_factor
        new_height = (new_height // scale_factor) * scale_factor
        return new_width, new_height

    @staticmethod
    def crop_image(img, target_width, target_height, reference_position):
        # Calculate crop box based on reference position
        width, height = img.size
        left = 0
        top = 0
        right = width
        bottom = height

        if width > target_width:
            if "Left" in reference_position:
                right = target_width
            elif "Right" in reference_position:
                left = width - target_width
            else:  # Center
                left = (width - target_width) // 2
                right = left + target_width

        if height > target_height:
            if "Top" in reference_position:
                bottom = target_height
            elif "Bottom" in reference_position:
                top = height - target_height
            else:  # Center
                top = (height - target_height) // 2
                bottom = top + target_height

        return img.crop((left, top, right, bottom))

    @staticmethod
    def scale_with_padding(img, target_width, target_height, reference_position, supersampling_method, padding):
        # First scale the image to fit within target dimensions while maintaining aspect ratio
        width, height = img.size
        aspect_ratio = width / height

        if width * target_height / height <= target_width:
            # Fit to height
            new_height = target_height
            new_width = int(target_height * aspect_ratio)
        else:
            # Fit to width
            new_width = target_width
            new_height = int(target_width / aspect_ratio)

        # Scale the image
        img = img.resize((new_width, new_height), ImageScaler.get_interpolation_method(supersampling_method))

        # Create a new image with padding
        new_img = Image.new("RGBA" if padding == "Transparent" else "RGB", 
                          (target_width, target_height), 
                          ImageScaler.get_padding_color(padding))

        # Calculate paste position based on reference position
        paste_x = 0
        paste_y = 0

        if "Left" in reference_position:
            paste_x = 0
        elif "Right" in reference_position:
            paste_x = target_width - new_width
        else:  # Center
            paste_x = (target_width - new_width) // 2

        if "Top" in reference_position:
            paste_y = 0
        elif "Bottom" in reference_position:
            paste_y = target_height - new_height
        else:  # Center
            paste_y = (target_height - new_height) // 2

        # Paste the scaled image
        new_img.paste(img, (paste_x, paste_y), img if padding == "Transparent" else None)
        return new_img

    @staticmethod
    def get_padding_color(padding):
        return {
            "Transparent": (0, 0, 0, 0),
            "Gray": (128, 128, 128),
            "Black": (0, 0, 0),
            "White": (255, 255, 255),
        }[padding]

    @staticmethod
    def get_interpolation_method(interpolation):
        interpolation_map = {
            "Nearest": Image.NEAREST,
            "Box": Image.BOX,
            "Bilinear": Image.BILINEAR,
            "Hamming": Image.HAMMING,
            "Bicubic": Image.BICUBIC,
            "Lanczos": Image.LANCZOS,
            "None": Image.NEAREST,
        }
        return interpolation_map[interpolation]

    @staticmethod
    def send_notification(message):
        print(f"[ImageScaler Notification]: {message}")


# Node registration
NODE_CLASS_MAPPINGS = {
    "ImageScaler": ImageScaler
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ImageScaler": "ImageScaler ğŸ¿"
}